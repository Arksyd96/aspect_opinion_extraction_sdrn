{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as et\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import io, json\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting java environ variable\n",
    "java_env_path = 'C:/Program Files/Java/jdk1.8.0_261/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_env_path\n",
    "\n",
    "# Stanford POS Tagger related constants\n",
    "tag_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS','NNP', 'NNPS', 'PDT', \n",
    "    'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP','SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', \n",
    "    'WP', 'WP$', 'WRB', 'HYPH', '-LRB-', '-RRB-', 'AFX', 'NFP', ',', '.', ':', '$', '#', \"``\", \"''\", '(',')']\n",
    "tag_idx = {tag: idx + 1 for idx, tag in enumerate(sorted(tag_list))}\n",
    "\n",
    "# Stanford POS Tagger\n",
    "tagger_dir = 'stanford-postagger/'\n",
    "tagger = StanfordPOSTagger( \n",
    "    os.path.join(tagger_dir, 'models/english-left3words-distsim.tagger'), \n",
    "    os.path.join(tagger_dir, 'stanford-postagger.jar'), \n",
    "    encoding='utf8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data_dir):\n",
    "    # placeholders\n",
    "    max_sequence_length = {}\n",
    "    data_length = {}\n",
    "    vocab = set()\n",
    "\n",
    "    # iterating over files\n",
    "    xml_files = os.listdir(data_dir)\n",
    "    for xml_file in xml_files:\n",
    "        root = et.parse(data_dir + xml_file).getroot()\n",
    "        for sentence in root.iter('sentence'):\n",
    "            text = sentence.find('text').text\n",
    "            tokens = word_tokenize(text)\n",
    "            [vocab.add(word) for word in tokens]\n",
    "            # updating metadata\n",
    "            max_sequence_length[xml_file] = max(max_sequence_length.get(xml_file, 0), len(tokens))\n",
    "            data_length[xml_file] = data_length.get(xml_file, 0) + 1\n",
    "\n",
    "    # sorting for more lisibility\n",
    "    vocab = sorted(vocab)\n",
    "    return {word: index + 1 for index, word in enumerate(vocab)}, max_sequence_length, data_length\n",
    "\n",
    "def build_data(vocab, source_file, max_sentence_length, data_length, save=True, out_dir='data/'):\n",
    "    # placeholders\n",
    "    x = np.zeros((data_length, 2, max_sentence_length)) # will contain indexes of words and pos_tags\n",
    "    y = np.zeros((data_length, max_sentence_length)) # will contain aspect_terms mask\n",
    "    opinions = []\n",
    "\n",
    "    # progress bar for more lisibility\n",
    "    pbar = tqdm(total=data_length)\n",
    "    \n",
    "    # iterating over each sentence of the data file\n",
    "    root = et.parse(source_file).getroot()\n",
    "    for s_idx, sentence in enumerate(root.iter('sentence')):\n",
    "        text = sentence.find('text').text\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = [tag_idx[tag] for _, tag in tagger.tag(tokens)]\n",
    "        for w_idx, word in enumerate(tokens):\n",
    "            x[s_idx, 0, w_idx] = vocab[word]\n",
    "            x[s_idx, 1, w_idx] = pos_tags[w_idx]\n",
    "\n",
    "        # iterating over each opinion of the given sentence\n",
    "        sentence_opinions = []\n",
    "        for opinion in sentence.iter('Opinion'):\n",
    "            target, category, polarity = opinion.get('target'), opinion.get('category'), opinion.get('polarity')\n",
    "            first, last = int(opinion.get('from')), int(opinion.get('to')) # first and last indexes of the opinion\n",
    "            if last != 0:\n",
    "                if first != 0:\n",
    "                    pre_seq_len = len(word_tokenize(text[:first]))\n",
    "                post_seq_len = len(word_tokenize(text[:last]))\n",
    "                # for training only identify aspect word, but not polarity\n",
    "                y[s_idx, pre_seq_len] = 1\n",
    "                if post_seq_len > pre_seq_len: # more than one token\n",
    "                    y[s_idx, pre_seq_len + 1: post_seq_len] = 2 \n",
    "                # adding to list\n",
    "                sentence_opinions.append({\n",
    "                    'target': target,\n",
    "                    'category': category,\n",
    "                    'polarity': polarity,\n",
    "                    'first': pre_seq_len,\n",
    "                    'last': post_seq_len\n",
    "                })\n",
    "        # adding to main opinions list\n",
    "        opinions.append(sentence_opinions)\n",
    "        # update pbar\n",
    "        pbar.update(1)\n",
    "    # if save is true, save the data\n",
    "    if save:\n",
    "        np.save(os.path.join(out_dir, 'train_x.npy'), x)\n",
    "        np.save(os.path.join(out_dir, 'train_y.npy'), y)\n",
    "        with open(os.path.join(out_dir, 'opinions.json')) as op_file:\n",
    "            json.dump(opinions, op_file)\n",
    "        print('Files saved at {}'.format(out_dir))\n",
    "    # return\n",
    "    return x, y, opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/raw/'\n",
    "\n",
    "vocab, max_sequence_length, data_size = build_vocab(data_dir)\n",
    "print('vocab size: {}'.format(len(vocab)))\n",
    "print('Max sequence length of each file: {}'.format(max_sequence_length))\n",
    "print('Sentences count of each file: {}'.format(data_size))\n",
    "\n",
    "train_file = 'ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "\n",
    "train_x, train_y, train_opinions = build_data(\n",
    "    vocab, data_dir + train_file, max_sequence_length[train_file], data_size[train_file],\n",
    "    save=True, out_dir='data/preprocessed/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: [('!', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), (\"''\", 7), (\"'after\", 8), (\"'best\", 9), (\"'cuz\", 10)] ...\n",
      "Train x shape: (2000, 2, 68)\n",
      "[[ 638. 2516. 3619. 3577. 4409. 4606. 4442. 1526. 1262. 2587. 3529.   25.\n",
      "  1687. 3282. 1395. 3022.   28.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [  39.   16.   17.   25.   12.   38.   35.   37.   12.   17.   22.    6.\n",
      "    10.   30.   30.   31.    8.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.]]\n",
      "Train y shape: (2000, 68)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Vocab: {} ...'.format(list(vocab.items())[:10]))\n",
    "print('Train x shape: {}\\n{}'.format(train_x.shape, train_x[0]))\n",
    "print('Train y shape: {}\\n{}'.format(train_y.shape, train_y[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "101b61497117ea3f18d4e0f8cf93eb2d64c16663f47aa00fa1289b89b66d7e41"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
